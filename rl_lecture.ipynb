{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Qué hemos visto hasta ahora en aprendizaje máquina:\n",
    "\n",
    " * Aprendizaje **supervisado**: Las observaciones, $X$, tienen asociada una etiqueta, $y$, y el objetivo es encontrar un función $f: X \\mapsto y$. Por ejemplo, clasificación y regresión.\n",
    " * Aprendizaje **no-supervisado**: Las observaciones , $X$, no tienen una etiqueta asociada y el objetivo es encontrar relaciones entre las observaciones. Por ejemplo, clustering y estimación de densidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aprendizaje **reforzado**: \n",
    "\n",
    "* Un agente interactua con el entorno a través de acciones y obtiene una recompensa. \n",
    "* El objetivo es maximizar esa recompensa. \n",
    "* La recompensa no tiene por que ser inmediata, por lo que un agente puede realizar varias acciones antes de recibir una recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ejemplos de aprendizaje reforzado:\n",
    "\n",
    "* Juegos - El agente, jugador, trata de ganar (recompensa positiva) sin perder (recompensa negativa). P.ej, AlphaGo Zero. \n",
    "* Invertir en valores - Ganar dinero (positivo), perder (negativo)\n",
    "* Planning - Elaborar una ruta. La ruta puede estar penalizada (negativa) por el número de pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ejemplos de recompensas:\n",
    "\n",
    "* Correr un maratón: El objetivo es llegar lo antes posible (penalización por tiempo). Alimentarse o hidratarse hace que tengamos una penalización a corto plazo, pero evitará que perdamos más tiempo a largo plazo.\n",
    "* Jugar al tenis: Hacer un punto (recompensa positiva), que te hagan un punto (negativa).\n",
    "* Estudiar para un examen: A corto plazo, estudiar puede tener una recompensa negativa porque no nos guste, pero a largo plazo aprobaremos el examen y no tendremos que estudiar (recompensa positiva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo el **agente** realiza la **interacción** con el **entorno**? La interacción se lleva a cabo en timesteps discretos, $ t = 0, 1, \\ldots $. En cada iteración se realizan las siguientes tareas:\n",
    "\n",
    "* El agente recibe una representación del estado actual del entorno, $s_t^a$,\n",
    "* que tiene la información necesaria para realizar la acción, $a_t$, donde $a_t$ pertenece al conjunto de acciones disponibles en el estado $s_t$, $a_t \\in A(s_t^a)$\n",
    "* y obtiene una recompensa $r_t$ ($r \\in \\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Representación gráfica de la interacción de un agente con el entorno.\n",
    "\n",
    "<img src=\"images/reinforcement_learning_diagram.png\" alt=\"rl diagram\" width=\"300\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El estado del entorno, $s_t^e$, puede ser igual o contener más información que el estado que recibe el agente, $s_t^a$, dando lugar a dos tipos de entornos:\n",
    " * **Observable**: El agente es capaz de percibir toda la información del entorno, $s_t^a = s_t^e$. Por ejemplo, en el ajedrez el entorno es el tablero y la colocación de las piezas, en este caso un agente (jugador) es capaz de percibir todo el entorno.\n",
    " * **Parcialmente observable**: El agente es capaz de observar solo una parte del entorno $s_t^a \\ne s_t^e$. Por ejemplo, en el juego flappy birds un agente (jugador) sólo percibe el entorno que se muestra en la pantalla, pero no tiene información sobre los obstaculos que hay después. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **historia** de un agente, $H_t$, es el secuencia de estados, acciones y recompensas hasta el instante $t$. $H_t = s_0, a_0, r_0, \\ldots, s_t, a_t, r_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Objetivo**: máximizar la secuencia cumulativa de recompensas:\n",
    "\n",
    "* Según la hipótesis de la recompensa (Rewards hypothesis), todos los objetivos pueden ser descritos como la maximización de la esperanza cumulativa de la recompensa. \n",
    "* En algunos casos es mejor sacrificar las recompensas a corto plazo para obtener una recompensa mayor a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov decision process\n",
    "\n",
    "* Un proceso de decision de Markov (MDP) proporciona un marco de trabajo matemático para sistemas de tomas de decisión donde hay aleatoriedad en las acciones tomadas. \n",
    "* Es un enfoque para algunos problemas de aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un MDP está compuesto por los siguientes componentes:\n",
    "\n",
    "* El estado, $S$,\n",
    "* El conjunto de acciones, $A$, y las acciones disponibles para el estado $s$, $A(s)$,\n",
    "* La probabilidad de transición $T(s,a,s\\prime) \\sim \\mathbb{P}(s\\prime|s,a)$, que es la probabilidad de acabar en el estado $s\\prime$ al realizar la acción $a$ en el estado $s$,\n",
    "* Y la recompensa $R(s,a,s\\prime)$ que es la recompensa esperada al pasar de $s$ a $s\\prime$ a través del la accción $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, se tiene que cumplir la propiedad de Markov:\n",
    "\n",
    "$\\mathbb{P}(S_{t+1}|S_t) = \\mathbb{P}(S_{t+1}|S_t, \\ldots, S_0)$\n",
    "\n",
    "que implica que el estado $s_t$ captura toda la información relevante de la historia (el **futuro** es independiente del **pasado** dado el **presente**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Objetivo**: Encontrar una policy $\\pi$ que permita seleccionar la acción a tomar, $a_t$, en el estado $s_t$ ($\\pi(s_t) = a_t$) que maximize una función de la secuencia cumulativa de recompensas.\n",
    "\n",
    "Normalmente, esta función de la secuencia cumulativa de recompensas es la esperanza de la suma de recompensas descontada sobre un horizonte infinito:\n",
    "\n",
    "$\\mathbb{E}[\\sum^\\infty_{t=0} \\gamma^t R(s_t,a_t,s_{t+1})]$\n",
    "\n",
    "Donde $\\gamma$ es el factor de descuento tal que $0 \\le \\gamma \\le 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El factor de descuento es un parámetro muy importante, que permite establecer cuan **lejos** el método tiene que mirar en el horizonte de recompensas:\n",
    "\n",
    "* Un factor de descuento cercano a cero indica que solo las recompensas **inmediatas** son consideradas, \n",
    "* Un factor de descuento a uno permite considerar un **horizonte mayor**. \n",
    "\n",
    "¿Cuál es el mejor factor de descuento? Como en la mayoría de los parámetros en ML, esto dependerá del problema. En la práctica, en la mayoría de los casos se elige $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Definiciones**: \n",
    "\n",
    "La policy óptima, $\\pi^*$, se representa como :\n",
    "\n",
    "$\\pi^* = argmax_\\pi \\mathbb{E} [\\sum^\\infty_{t=0} \\gamma^t R(s_t) |\\pi]$\n",
    "\n",
    "Con esta información podemos definir la función Value, $V(s)$, que define la recompensa esperada en el futuro empezando desde el estado $s$ siguiendo la política $\\pi$:\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}[\\sum^\\infty_{t=0} \\gamma^t R(s_t) |\\pi, s_0=s]$\n",
    "\n",
    "y podemos escribir la política óptima en términos de $V(s)$:\n",
    "\n",
    "$\\pi^* = argmax_a \\sum_{s\\prime} T(s, a, s\\prime) * V^{\\pi^*} (s\\prime)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos para resolver problemas de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para visualizar como funcionan los algoritmos que vamos a ver con un problema de aprendizaje reforzado llamado FrozenLake-v0:\n",
    "\n",
    "<img src=\"images/frozen_lake.png\" alt=\"frozen lake\" width=\"300\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Descripción del juego**: \n",
    "* caminar por un lago helado (F) para llegar a la meta (G), \n",
    "* evitando caerse al agua en las zonas donde la capa de hielo es menor (nos caeríamos) (H). \n",
    "* los nodos terminales, que indican que el juego se acaba, son G y H.\n",
    "* Las acciones que puede realizar el agente son moverse a la izquierda, derecha, arriba y abajo. \n",
    "* El número de estados son 16, y representa cada una de las celdas del grid.\n",
    "\n",
    "Fuente imagen: \n",
    "https://academy.dataiku.com/latest/tutorial/machine-learning/reinforcement-learning-q-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Breve introduducción al paquete gym de Openai**\n",
    "\n",
    "Lo primero que tenemos que hacer es crear un entorno (ver otros entornos en https://gym.openai.com/), y resetearlo para llevarlo al estado original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para visualizar el entorno utilizamos la función render. Esta función tiene un parámetro **mode** que nos permitirá utilizar diferentes tipos de representaciones como texto o gráfica, en el entorno frozenlake solo tenemos disponible la representación basada en texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El conjunto de posibles acciones se encuentra en el atributo **action_space**. En FrozenLake, solo disponemos de cuatro acciones: izquierda, abajo, derecha y arriba (0, 1, 2, 3) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de posibles estados se encuentra el atributo **nS**. En este problema son 16 (es un grid de 4x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para realizar una acción utilizamos el método **step**, que nos devuelve el nuevo estado, recompensa, un flag que nos indica si el juego ha terminado o no, e información sobre la transición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.0 False\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2) \n",
    "print(state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob': 0.3333333333333333}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos de nuevo el entorno veremos como el agente se ha movido de la celda inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez terminado el entrenamiento podemos cerrar el entorno con el método **close**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programación dinámica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dp_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Value iteration es un método para calcular la policy y funcion value óptima para un MDP,\n",
    "* Este método parte de la premisa que la función value, $V(s)$, es suficiente para aprender la policy óptima. \n",
    "* Por lo tanto, este método primero cálcula $V(s)$, y posteriormente $\\pi^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El método de value iteration utiliza la definición de $V(s) \\equiv V^\\pi(s)$, con las definiciones anteriores llegando a:\n",
    "\n",
    "$V(s) = R(s) + \\gamma max_a \\sum_{s\\prime} T(s, a, s\\prime) V(s\\prime)$\n",
    "\n",
    "Con esta definición de $V(s)$, value iteration se define como:\n",
    "\n",
    "$V_{t+1}(s) = R(s) + \\gamma max_a \\sum_{s\\prime} T(s, a, s\\prime) V_t(s\\prime)$\n",
    "\n",
    "donde se empieza con una función valor aleatoria, $V_0$, y se actualiza hasta que converja. En la práctica, el algoritmo se detiene cuando los cambios de la función valor son menores a un umbral, en este caso theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El psudo-código de value iteration es el siguiente (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/value_iteration.png\" alt=\"value iteration\" width=\"600\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(env, state, V, discount_factor):\n",
    "        \"\"\"\n",
    "        CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "        \n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            A[a] = sum([prob * (reward + discount_factor * V[next_state]) \n",
    "                        for prob, next_state, reward, done in env.P[state][a]])\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, return_history=True):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "\n",
    "    V_history = []\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(env, s, V, discount_factor)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value\n",
    "        \n",
    "        if return_history:\n",
    "            V_history.append(V.copy())\n",
    "            \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(env, s, V, discount_factor)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    if return_history:\n",
    "        return policy, V, V_history\n",
    "    else:\n",
    "        return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de value iteration en custom frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcf2d6efb6e451aa5bfa187f75429c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_display(train_utils.display_lake_policy, None, train_utils.display_lake_value_history)\n",
    "train_utils.display_learning_widget(value_iteration, dp_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a probar a jugar una partida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_env = gym.make('custom-FrozenLake-v0', prob_action=0.8, rg_hole=0, rg_floor=0)\n",
    "p_frozen, v_frozen = value_iteration(frozen_env, return_history=False, discount_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards 1\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(frozen_env, p_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>↓</td><td>↑</td><td>↑</td><td>↑</td></tr>\n",
       "<tr><td>←</td><td>H</td><td>↑</td><td>H</td></tr>\n",
       "<tr><td>↑</td><td>↓</td><td>←</td><td>H</td></tr>\n",
       "<tr><td>H</td><td>→</td><td>↓</td><td>G</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_utils.display_lake_policy(frozen_env, p_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa7179c0990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_utils.display_lake_value(frozen_env, v_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration es un método para calcular la política óptima para un MDP. El algoritmo está compuesto por dos métodos:\n",
    "\n",
    "* policy evaluation: se calcula la función value (también llamada state-value) para una policy arbitraria.\n",
    "* policy improvement: se mejora la policy utilizando la función value del paso anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Evaluation**:\n",
    " * Calculamos la función value utilizando una policy $\\pi$ con las siguientes ecuaciones:\n",
    "\n",
    "$V_\\pi(s) = \\mathbb{E}[G_t | S_t = s]$\n",
    "$= \\mathbb{E}[R_{t+1} + \\gamma V_\\pi(s_{t+1}) | S_t = s]$\n",
    "$= \\sum_{a} \\pi(a|s) \\sum_{s\\prime, r} p(s, a, s\\prime) * [r + \\gamma V(s\\prime)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvement**:\n",
    "\n",
    "* Una vez hemos calculado la función value $V_\\pi(s)$ podemos utilizarla para mejorar la policy. \n",
    "* Para mejorar la política, se compara la acción seleccionada para un estado usando la política con la mejor acción utilizando la función valor, y en caso de ser diferentes, se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El pseudo-código de policy iteration es el siguiente (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/policy_iteration.png\" alt=\"policy iteration\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.001):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the current policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(env, s, V, discount_factor)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de policy iteration en custom frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74848d8aabbd49fea766a5eb813523fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_display(train_utils.display_lake_policy, None, train_utils.display_lake_value_history)\n",
    "train_utils.display_learning_widget(policy_improvement, dp_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo\n",
    "\n",
    "* Los métodos vistos hasta ahora de programación dinámica tienen una gran limitación, ya que asumen que se tiene un conocimiento completo del entorno,\n",
    "* En la práctica, esto no es siempre es así.\n",
    "* Los métodos de monte carlo solucionan este problema aprendiendo del entorno a través de la experiencia generada al interaccionar con el entorno.\n",
    "* Estos métodos tienen una gran varianza, pero no está sesgados. \n",
    "* Convergencia asintótica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo se definen la experiencia?\n",
    "\n",
    "* La experiencia se agrupa en episodios (en el caso de los juegos, un episodio podría ser una partida),\n",
    "* cada episodio esta compuesto de una secuencia ordenada de la 4-upla (estado, acción, recompensa y proximo estado)\n",
    "* utilizando esta experiencia, podemos calcular la función state-action\n",
    "* y mejorarla en cada episodio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo estimamos la policy?\n",
    "\n",
    "* Generamos una función state-action $Q(s,a)$ aleatoria,\n",
    "* y creamos una policy $\\pi$ utilizando $Q(s,a)$,\n",
    "* Posteriormente, generamos un episodio siguiendo la policy $\\pi$,\n",
    "* Para cada estado y acción $s, a$ en el episodio, buscamos la primera vez que aparecen en el episodio,\n",
    "* y calculamos la recompensa cumulativo desde ese punto,\n",
    "* actualizamos $Q(s, a)$ con la media de todas las recompensas cumulativas para $s, a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Psuedo código del método montecarlo para estimar $\\pi$ (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/montecarlo.png\" alt=\"montecarlo\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, episode_steps=100, discount_factor=1.0, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(1, num_episodes + 1)):\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(episode_steps):\n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "                \n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mc_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'},\n",
    "    'epsilon': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'epsilon'},\n",
    "    'num_episodes': {'values': (10**3, 10**4, 3 * 10**4, 6 * 10**4, 10**5, 10**6, 10**7), 'default': 10**4, 'text': 'num_episodes'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de Monte Carlo en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2f890ffc094fbdb526fb544ead1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(mc_control_epsilon_greedy, mc_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporal-difference Learning\n",
    "\n",
    "* TD-Learning es una combinación de las técnicas Monte Carlo y programación dinámica,\n",
    "* Como montecarlo, TD funciona con las experiencias y no requiere de un modelo del entorno\n",
    "* Al igual que la programación dinámica, TD utiliza bootstrapping para hacer las actualizaciones.\n",
    "* La convergencia de TD suele aprender más rápido pero están sesgados (a las condiciones iniciales),\n",
    "* Por otra parte, los métodos basados en Monte Carlo no están sesgados pero requieren de más iteraciones para llegar a un resultado similar a TD.\n",
    "\n",
    "<!-- \n",
    "The main problem with TD learning and DP is that their step updates are biased on the initial conditions of the learning parameters. The bootstrapping process typically updates a function or lookup Q(s,a) on a successor value Q(s',a') using whatever the current estimates are in the latter. Clearly at the very start of learning these estimates contain no information from any real rewards or state transitions.\n",
    "\n",
    "If learning works as intended, then the bias will reduce asymptotically over multiple iterations. However, the bias can cause significant problems, especially for off-policy methods (e.g. Q Learning) and when using function approximators. That combination is so likely to fail to converge that it is called the deadly triad in Sutton & Bart.\n",
    "\n",
    "Monte Carlo control methods do not suffer from this bias, as each update is made using a true sample of what Q(s,a) should be. However, Monte Carlo methods can suffer from high variance, which means more samples are required to achieve the same degree of learning compared to TD.\n",
    "\n",
    "In practice, TD learning appears to learn more efficiently if the problems with the deadly triad can be overcome. Recent results using experience replay and staged \"frozen\" copies of estimators provide work-arounds that address problems - e.g. that is how DQN learner for Atari games was built. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las actualizaciones en TD-Learning tienen normalmente la siguiente forma:\n",
    "    \n",
    "* $V(s_t) = V(s_t) + \\alpha[R_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})]$,\n",
    "* Estimacion = Valor actual + $\\alpha$ [Nuevo valor - Valor actual],\n",
    "* donde $0 \\le \\alpha \\le 1$  es el parámetro de aprendizaje que regula las actualizaciones.\n",
    "* El término $R_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$ es el \"error\", de la estimación $V(s_{t})$ y la mejor estimación $R_{t+1} + \\gamma V(s_{t+1})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En las siguientes secciones vamos a ver dos métodos basados en temporal-difference, que tienen la siguiente forma:\n",
    "\n",
    "* El objetivo es calcular la función state-action ($Q(S, A)$)\n",
    "* El método se repite para un cojunto de episodios predeterminado:\n",
    "* Dentro del episodio, el método escoge una acción $a$ utilizando una policy derivada de $Q(S, A)$.\n",
    "* Ejecuta la acción, obteniendo el nuevo estado $s\\prime$, y la recompensa $r$,\n",
    "* y actualizan $Q(S, A)$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las diferencias de los dos métodos radican en cómo se actualiza $Q(S, A)$:\n",
    "\n",
    "* SARSA: Es un método on-policy. La actualización de los q-values se realiza utilizando la policy actual.\n",
    "* Q-Learning: Es un método off-policy. La actualización de los q-values se realiza utilizando el siguiente estado $s\\prime$ y la acción $a\\prime$ utilizando un enfoque greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "td_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'},\n",
    "    'alpha': {'values': np.arange(0, 1.1, 0.1), 'default': 0.5, 'text': 'alpha'},\n",
    "    'epsilon': {'values': np.arange(0, 1.1, 0.1), 'default': 0.1, 'text': 'epsilon'},\n",
    "    'num_episodes': {'values': (10**3, 10**4,  3 * 10**4, 6 * 10**4, 10**5, 10**6, 10**7), 'default': 10**4, 'text': 'num_episodes'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SARSA\n",
    "\n",
    "* SARSA es un método on-policy, por lo que la función $Q(S,A)$ se actualiza utilizando la policy actual.\n",
    "* En la actualización, el método elige una acción $a\\prime$ para el estado $s\\prime$ utilizando la policy derivada de $Q(S,A)$,\n",
    "* y actualiza $Q(s,a) =  Q(s, a) + \\alpha[R + \\gamma Q(s\\prime, a\\prime) - Q(s, a)]$.\n",
    "* Recordar que tiene una forma similar al \"error\" en TD.\n",
    "* La acción $a\\prime$ -> $a$, y $s\\prime$ -> $s$, y pasamos a la siguiente iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Psuedo código del método SARSA (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/sarsa.png\" alt=\"sarsa\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-Learning es un método off-policy, por lo que la función $Q(S,A)$ se actualiza utilizando un enfoque greedy.\n",
    "* En la actualización, el método elige una acción $a\\prime$ para el estado $s\\prime$ utilizando un enfoque greedy,\n",
    "* y actualiza $Q(s,a) =  Q(s, a) + \\alpha[R + \\gamma max_{a\\prime} Q(s\\prime, a\\prime) - Q(s, a)]$.\n",
    "* Recordar que tiene una forma similar al \"error\" en TD.\n",
    "* El estado $s\\prime$ -> $s$, y pasamos a la siguiente iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Psuedo código del método q learning (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/qlearning.png\" alt=\"q learning\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementaciones de SARSA y Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        if encode_state:\n",
    "            state = encode_state(state)\n",
    "            \n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "            \n",
    "            # Pick the next action\n",
    "            next_action_probs = policy(next_state)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "            \n",
    "            # TD Update\n",
    "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            action = next_action\n",
    "            state = next_state        \n",
    "\n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de sarsa en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a71a8afe9c4dccabeb4739cc190f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(sarsa, \n",
    "                                    td_params, \n",
    "                                    plot_func, \n",
    "                                    env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance to sample a random action. Float between 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "\n",
    "        if encode_state:\n",
    "            state = encode_state(state)\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in itertools.count():        \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                 \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state   \n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de Q-Learning en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41649b0137494b34bb88d666dfcc53f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(q_learning, td_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros entornos de gym para prácticar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**:\n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    \n",
    "**Observations**: \n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "    \n",
    "**Passenger locations**:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "    \n",
    "**Destinations**:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "        \n",
    "**Actions**:\n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger\n",
    "    \n",
    "**Rewards**: \n",
    "There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_env.render()\n",
    "taxi_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis utilizar la función cached_call que persite las ejecuciones entre sesiones para probar diferentes combinaciones, tiene como parámetros la función que vais utilizar para resolver el problema, el nombre del entorno, un diccionario con los argumentos para crear el entorno, y otro diccionario con los parámetros del método. El resultado es una tupla con el resultado/s del método, y el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c994182ecb4446f9c9bf9e3e89d1025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_mc, taxi_env = train_utils.cached_call(q_learning, 'Taxi-v3', {}, method_kwargs={'num_episodes':10**4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podéis utilizar la función directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_mc = q_learning(taxi_env, num_episodes=10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reproducir un episodio podéis utilizar la función play_episode, que recibe como parámetro el entorno y una policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards 20\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(taxi_env, train_utils.get_policy_from_q(Q_mc, taxi_env.nS, taxi_env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver descripción del entorno en https://gym.openai.com/envs/MountainCar-v0/ y https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_env = gym.make('MountainCar-v0')\n",
    "car_env.reset()\n",
    "car_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCairStateEncoder:\n",
    "    \n",
    "    def __init__(self, env, num_buckets_v = 20, num_buckets_x = 20):\n",
    "        self.env = env\n",
    "        self.num_buckets_v = num_buckets_v\n",
    "        self.num_buckets_x = num_buckets_x\n",
    "        \n",
    "        self.min_x = env.unwrapped.min_position\n",
    "        self.max_x = env.unwrapped.max_position\n",
    "        \n",
    "        self.min_v = -env.unwrapped.max_speed\n",
    "        self.max_v = env.unwrapped.max_speed\n",
    "    \n",
    "        self.nS = self.num_buckets_v * self.num_buckets_x\n",
    "        \n",
    "    def get_bucket(self, vmin, vmax, v, num_buckets):\n",
    "        bucket_size = (vmax - vmin) / num_buckets\n",
    "        return int((v - vmin) // bucket_size)\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        x, v = state\n",
    "        bucket_x = self.get_bucket(self.min_x, self.max_x, x, self.num_buckets_x)\n",
    "        bucket_v = self.get_bucket(self.min_v, self.max_v, v, self.num_buckets_v)\n",
    "        return int(bucket_x * self.num_buckets_v + bucket_v)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.num_buckets_v, self.num_buckets_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac493e5b964e419eea2e42d461dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "car_state_encoder = MountainCairStateEncoder(car_env)\n",
    "Q_car, car_env = train_utils.cached_call(q_learning, 'MountainCar-v0', {}, {'num_episodes': 3 * 10**3, 'encode_state':car_state_encoder})\n",
    "p_car = train_utils.get_policy_from_q(Q_car, car_state_encoder.nS, car_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards -1.0\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(car_env, p_car, encode_state=car_state_encoder, max_steps=2500)\n",
    "car_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción del entorno en https://gym.openai.com/envs/CartPole-v1/ y https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias:\n",
    "\n",
    "* Libro de Sutton & Barto, no es muy complejo y está abierto en: http://incompleteideas.net/book/bookdraft2017nov5.pdf\n",
    "* Diapositivas de Silver: https://www.davidsilver.uk/teaching/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
